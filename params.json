{
  "name": "Scrapy spider",
  "tagline": "A Directory Stored with Multiple Spiders",
  "body": "# **Spider Constructed by scrapy**\r\n## 1. Create your Spider Project\r\nOpen bash (linux) or cmd (Windows)\r\n\r\n```\r\n$ active snakes # launch py2.7 env\r\n```\r\n\r\nScrapy now can only work perfectly with python2.x, so we activate the env of python2.7.\r\n\r\n```\r\n$ scrapy startproject xkcd\r\n```\r\n\r\nThus, we 've already generated a Project named xkcd, we are going to make it more powerful.\r\n\r\n```\r\n$ cd xkcd\r\n```\r\n\r\n**Note :** you must change to xkcd dir, otherwise the project can't execute.\r\n\r\n## 2. Define your Structured Data\r\nEdit`.\\items.py` to define fields of scraped items like this.\r\n\r\n```\r\nimport scrapy\r\n\r\n\r\nclass XkcdItem(scrapy.Item):\r\n\r\n    # define the fields for your item here like:\r\n    name = scrapy.Field()\r\n    image_urls = scrapy.Field()\r\n    desc = scrapy.Field()\r\n    file_paths = scrapy.Field()\r\n```\r\n\r\nThe example code define a structured data form like this:\r\n<center>\r\n<table>\r\n<tr><td><b>name</b></td><td><b>owner</b></td><td><b>price</b></td>\r\n</tr><tr><td>Coffee</td><td>John</td><td>$ 10</td></tr><tr><td>...</td><td>...</td><td>...</td></tr>\r\n</table>\r\n</center>\r\n\r\nLater, we will use well-defined XkcdItem class to instantiate items with scraped data in `xkcd_spider.py`\r\n\r\n## 3. Create your Spiders\r\nDefine the spiders' behavious to extract the data you wanted. A example to scrape all the link of images in XKCD.com\r\n\r\n```\r\nimport os\r\nimport scrapy\r\nfrom xkcd.items import XkcdItem\r\nfrom scrapy.spiders import Spider\r\nfrom scrapy.http import Request\r\n\r\n\r\nclass XkcdSpider(scrapy.Spider):\r\n    name = 'xkcd'\r\n    allowed_domains = [\"xkcd.com/\"]\r\n    start_urls = [\"http://xkcd.com\"] + [\"http://xkcd.com/%d/\" % num for num in xrange(1, 4)]\r\n\r\n    def parse(self, response):\r\n\r\n        # Code Here to instantiate item\r\n        item = XkcdItem()\r\n        item['name'] = response.css('div#comic > img::attr(\"alt\")').extract()\r\n        item['image_urls'] = [\"http:\" + response.css('div#comic > img::attr(\"src\")').extract()[0]]\r\n        item['desc'] = response.css('div#comic > img::attr(\"title\")').extract()\r\n\r\n        yield item\r\n```\r\n\r\nWell Done! we 've had a spider to work for us for get wanted item filled with scraped data.\r\n\r\n## 4. Customize File Pipelines to Stroe Data\r\n* ### FilesPipeline\r\n```\r\ndef file_path(self, request, response=None, info=None):\r\n      if re.search(r'\\d', request.url):\r\n          media_guid = re.search(r'\\d+', request.url).group()\r\n      else:\r\n          media_guid = '0'\r\n      return 'web/%s.html' % (media_guid)\r\n```\r\n* ### ImagesPipeline\r\n```\r\nfrom scrapy.pipelines.images import ImagesPipeline\r\nfrom scrapy.exceptions import DropItem\r\n\r\n\r\nclass XkcdImgPipeline(ImagesPipeline):\r\n    def file_path(self, request, response=None, info=None,):\r\n        image_guid = request.url.split('/')[-1]\r\n        return 'images/%s' % (image_guid)\r\n\r\n    def get_media_requests(self, item, info):\r\n        for image_url in item['image_urls']:\r\n            yield scrapy.Request(image_url)\r\n\r\n    def item_completed(self, results, item, info):\r\n        image_paths = [x['path'] for ok, x in results if ok]\r\n        if not image_paths:\r\n            raise DropItem(\"Item contains no images\")\r\n        item['image_paths'] = image_paths\r\n        yield item\r\n  ```\r\n\r\n## 5. Configure Setting of Project\r\n:sunglasses:\r\n\r\n`$ scrapy crawl xkcd --logfile=\"xkcd_log.log\" -o xkcd.json -t json`\r\n\r\n`$ scrapy crawl xkcd -a end=10`\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}